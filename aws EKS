=============================================================================================
EKS
=============================================================================================
Cluster creation
	- EKS cluster can be created manually via Console.
	- EKS cluster can be created usin eksctl (cmdline interface).

1> Manual Cluster Creation (Console)
Cluster policy 
	- AmazonEKSClusterPolicy

Node policy
	- AmazonEKSWorkerNodePolicy
	- AmazonEKS_CNI_Policy
	- AmazonEC2ContainerRegistryReadOnly

Note - we can also create Fargate to deploy pods (Nat gateway s required - private subnets)

To access node via linux machine

- install docker
	- sudo apt install docker.io
	- docker --version

- install awscli
	- sudo apt install awscli
	- aws --version

- configure credentials 
	- aws configure
	- giv access key ID, secret access key, region and output format.

- install kubectl based on the selected kubernetes version while creatin the cluster
	- https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
	
Kubernetes 1.23
step 1 - curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl
step 2 - curl -o kubectl.sha256 https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl.sha256
step 3 - openssl sha1 -sha256 kubectl
step 4 - chmod +x ./kubectl
step 5 - mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
step 6 - echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
step 7 - kubectl version --short --client

Integrate linux machine with the Cluster

	- aws eks --region <region-code> describe-cluster --name <name_of_the_cluster> --query cluster.status // gives status of the cluster 
		eg:- aws eks --region us-east-1 describe-cluster --name namma-eks-cluster --query cluster.status //you will hav to get the console output as "Active"
		
	- aws eks update-kubeconfig --region <region-code> --name <name_of_the_cluster> // updates the present cluster configuration details to config file
		eg:- aws eks update-kubeconfig --region us-east-1 --name namma-eks-cluster
	
	- aws eks update-kubeconfig --name namma-eks-cluster --region us-east-1 // update the present policy cluster configuration to config file
		eg:- aws eks update-kubeconfig --name namma-eks-cluster --region us-east-1 --role-arn arn:aws:iam::505883253114:role/namma-eks-cluster-policy-role
	
	aws sts get-caller-identity // To see the configuration of your AWS CLI user or role, run the following command
	aws eks update-kubeconfig --name eks-cluster-name --region aws-region // Update or generate the kubeconfig file using one of the following commands
	aws eks update-kubeconfig --name eks-cluster-name --region aws-region --role-arn arn:aws:iam::XXXXXXXXXXXX:role/testrole // As the IAM role, run the following command
	kubectl config view --minify    // To confirm that the kubeconfig file is updated, run the following command
	kubectl get svc    // To confirm that your IAM user or role is authenticated, run the following command


2> EKS cluster usin eksctl (cmdline interface)
- Install eksctl on linux machine
	- curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
	- sudo mv /tmp/eksctl /usr/local/bin
	- eksctl version

Cluster creation
- Managed nodes
	- eksctl create cluster --name <cluster_name> --region <region-code>
		eg:- eksctl create cluster --name namma-eks-cluster --region us-east-1

- Fargate
	- eksctl create cluster --name <cluster_name> --region <region-code> --fargate
		eg:- eksctl create cluster --name namma-eks-cluster --region us-east-1 --fargate
		
- Delete Cluster
	- eksctl delete cluster --name <cluster_name> --region <region-code>				
		eg:- eksctl delete cluster --name namma-eks-cluster --region us-east-1


eksctl create cluster \
--name namma-eks-cluster \
--version 1.23 \
--region us-east-1 \
--nodegroup-name namma-eks-node \
--node-type t3.medium \
--nodes 2
--nodes-min 2 \
--nodes-max 4 \
--ssh-access \
--ssh-public-key mykey

--vpc-public-subnets subnet-public-ID1,subnet-publicI-D2 \
--vpc-private-subnets subnet-private-ID1,subnet-private-ID2 \

To update a node group version
eksctl upgrade nodegroup \
--name=namma-eks-node \
--cluster=namma-eks-cluster \
--kubernetes-version=1.23

Node taints on managed node groups
aws eks create-nodegroup \
 --cli-input-json '
{
  "clusterName": "namma-eks-cluster",
  ...
  "taints": [
     {
         "key": "color",
         "value": "red",
         "effect": "NO_SCHEDULE"
     }
   ],
}'
Note: - The effect must be one of No_Schedule, Prefer_No_Schedule, or No_Execute.

To delete a managed node group
eksctl delete nodegroup \
  --cluster namma-eks-cluster \
  --region us-east-1 \
  --name namma-eks-node

Launch Node Configuration
Amazon Linux
eksctl create nodegroup \
  --cluster namma-eks-cluster \
  --name namma-eks-node \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 4 \
  --ssh-access \
  --managed=false \
  --ssh-public-key mykey
eksctl create nodegroup --help

Windows 
eksctl create nodegroup \
    --region us-east-1 \
    --cluster namma-eks-cluster \
    --name namma-eks-node \
    --node-type t3.medium \
    --nodes 3 \
    --nodes-min 1 \
    --nodes-max 4 \
    --managed=false \
    --node-ami-family WindowsServer2019FullContainer
eksctl command -help

AWS Fargate
To create a Fargate profile
eksctl create fargateprofile \
    --cluster namma-eks-cluster \
    --name namma-fargate-profile \
    --namespace namma-kubernetes-namespace \
    --labels color=red // key=value

Deleting a Fargate profile
eksctl delete fargateprofile  --name namma-fargate-profile --cluster namma-eks-cluster
	
